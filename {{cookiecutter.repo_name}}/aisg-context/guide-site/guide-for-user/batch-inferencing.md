# Batch Inferencing

Some problem statements do not warrant the deployment of an API server
but instead methods for conducting batched inferencing where a batch
of data is submitted to say a script and the script churns out
a set of predictions, perhaps exported to a file.

This template provides a Python script (`src/batch_inferencing.py`)
as well as an accompanying
Dockerfile (`docker/{{cookiecutter.repo_name}}-batch-inferencing.Dockerfile`)
for containerised executions.

To execute the script locally:

```bash
$ python src/batch_inferencing.py \
  inference.model_path=<PATH_TO_MODEL> \
  inference.input_data_dir=<PATH_TO_DIR_CONTAINING_TXT_FILES>
```

The parameter `inference.input_data_dir` assumes a directory
containing `.txt` files containing movie reviews. At the end of the
execution, the script will log to the terminal the location of the
`.jsonl` file (`batch-infer-res.jsonl`) containing predictions that
look like such:

```jsonl
...
{"time": "2022-01-06T06:40:27+0000", "filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/1131_2.txt", "logit_prob": 0.006387829780578613, "sentiment": "negative"}
{"time": "2022-01-06T06:40:27+0000", "filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/11020_3.txt", "logit_prob": 0.0041103363037109375, "sentiment": "negative"}
{"time": "2022-01-06T06:40:27+0000", "filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/11916_3.txt", "logit_prob": 0.023626357316970825, "sentiment": "negative"}
{"time": "2022-01-06T06:40:27+0000", "filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/3129_2.txt", "logit_prob": 0.00018364191055297852, "sentiment": "negative"}
{"time": "2022-01-06T06:40:27+0000", "filepath": "/home/aisg/{{cookiecutter.repo_name}}/data/2444_4.txt", "logit_prob": 3.255962656112388e-05, "sentiment": "negative"}
...
```

The inferencing results are exported to a subdirectory within the
`outputs` folder. See
[here])(https://hydra.cc/docs/tutorials/basic/running_your_app/working_directory/)
for more information on outputs generated by Hydra.

To use the Docker image, first build it:

```bash
$ docker build \
  -t asia.gcr.io/{{cookiecutter.gcp_project_id}}/batch-inference:0.1.0 \
  --build-arg PRED_MODEL_UUID="abf043e8a8504eddb1f95bdbc634d2bd" \
  -f docker/{{cookiecutter.repo_name}}-batch-inferencing.Dockerfile .
```

Similar to how the predictive models are defined for the FastAPI
servers' images, `PRED_MODEL_UUID` requires the unique ID associated
with the MLflow run that generated the predictive model that you wish
to make use of for the batch inferencing.

After building the image, you can run the container like so:

```bash
$ chgrp -R 2222 outputs
$ docker run --rm \
  --env GOOGLE_APPLICATION_CREDENTIALS=/var/secret/cloud.google.com/gcp-service-account.json \
  --env INPUT_DATA_DIR=/home/aisg/{{cookiecutter.repo_name}}/data \
  -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \
  -v $PWD/models:/home/aisg/from-gcs \
  -v $PWD/outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs \
  -v <PATH_TO_DIR_CONTAINING_TXT_FILES>:/home/aisg/{{cookiecutter.repo_name}}/data \
  asia.gcr.io/{{cookiecutter.gcp_project_id}}/batch-inference:0.1.0
```

In Docker run command above we are passing two variables:
`GOOGLE_APPLICATION_CREDENTIALS` and `INPUT_DATA_DIR`.
The former allows the container's entrypoint to download the
predictive model specified during build time from GCS. The latter
will be fed to the script's parameter: `inference.input_data_dir`.
4 volumes are attached to the container for persistence as well as
usage of host files and directories.

- `-v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json`:
  This attaches the JSON file for the service account credentials to
  the Docker container.
- `-v $PWD/models:/home/aisg/from-gcs`: The models downloaded to the
  host machine can be used by the container after being mounted to
  `/home/aisg/from-gcs`.
- `-v $PWD/outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs`:
  This is for persisting the batch inferencing outputs to the outputs
  folder on the host machine.
- `-v <PATH_TO_DIR_CONTAINING_TXT_FILES>:/home/aisg/{{cookiecutter.repo_name}}/data`:
  To provide the container with access to the data that is on your local
  machine, you need to mount the directory containing the text
  files for inferencing.

Reference(s):

- [Docker Docs - Use volumes](https://docs.docker.com/storage/volumes/)
